{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce costs with Elastic Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create this scriptâ€¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch, torchvision\n",
      "import subprocess\n",
      "\n",
      "# Toggle inference mode\n",
      "model = torchvision.models.densenet121(pretrained=True).eval()\n",
      "cv_input = torch.rand(1,3,224,224)\n",
      "model = torch.jit.trace(model,cv_input)\n",
      "torch.jit.save(model, 'model.pt')\n",
      "subprocess.call(['tar', '-czvf', 'densenet121_traced.tar.gz', 'model.pt'])\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat create_sm_tarball.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And and empty file call script.py as placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = 'us-west-2'\n",
    "role = 'arn:aws:iam::921212210452:role/service-role/AmazonSageMaker-ExecutionRole-20191122T164449'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload tarball to S3\n"
     ]
    }
   ],
   "source": [
    "instance_type = 'c5.large'\n",
    "accelerator_type = 'eia2.medium'\n",
    "\n",
    "ecr_image = '763104351884.dkr.ecr.{}.amazonaws.com/pytorch-inference-eia:1.3.1-cpu-py3'.format(region)\n",
    "\n",
    "# Satisfy regex\n",
    "endpoint_name = 'pt-ei-densenet121-tracedV-{}-{}'.format(instance_type, accelerator_type).replace('.', '').replace('_', '')\n",
    "tar_filename = 'densenet121_traced.tar.gz'\n",
    "\n",
    "# script.py should be blank to use default EI model_fn and predict_fn\n",
    "# For non-EI PyTorch usage, must implement own model_fn\n",
    "entry_point = 'script.py'\n",
    "\n",
    "# Returns S3 bucket URL\n",
    "print('Upload tarball to S3')\n",
    "model_data = sagemaker_session.upload_data(path=tar_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pytorch = PyTorchModel(framework_version='1.4.0', model_data=model_data, \n",
    "                role=role, image=ecr_image, entry_point=entry_point, sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Function will exit before endpoint is finished creating\n",
    "predictor = pytorch.deploy(initial_instance_count=1, instance_type='ml.' + instance_type, \n",
    "                accelerator_type='ml.' + accelerator_type, endpoint_name=endpoint_name, wait=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchPredictor\n",
    "import torch\n",
    "import boto3\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "instance_type = 'c5.large'\n",
    "accelerator_type = 'eia2.medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing warmup round of 100 inferences (not counted)\n",
      "Running 1000 inferences for pt-ei-densenet121-tracedV-c5large-eia2medium:\n",
      "Client end-to-end latency percentiles:\n",
      "Avg | P50 | P90 | P95 | P100\n",
      "65.0304 | 63.2639 | 72.3619 | 76.7848\n",
      "\n",
      "Getting Cloudwatch:\n",
      "Time elapsed: 365.033259 seconds\n",
      "Using period of 420 seconds\n",
      "\n",
      "Waiting 30 seconds ...\n",
      "740.0 latency datapoints ready\n",
      "Side-car latency percentiles:\n",
      "Avg | P50 | P90 | P95 | P100\n",
      "50.4217 | 49.1494 | 57.2891 | 61.2198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'pt-ei-densenet121-tracedV-{}-{}'.format(instance_type, accelerator_type).replace('.', '').replace('_', '')\n",
    "predictor = PyTorchPredictor(endpoint_name)\n",
    "data = torch.rand(1,3,224,224)\n",
    "\n",
    "# Do warmup round of 100 inferences to warm up routers\n",
    "print('Doing warmup round of 100 inferences (not counted)')\n",
    "for i in range(100):\n",
    "  output = predictor.predict(data)\n",
    "time.sleep(15)\n",
    "\n",
    "client_times = []\n",
    "print('Running 1000 inferences for {}:'.format(endpoint_name))\n",
    "cw_start = datetime.datetime.utcnow()\n",
    "for i in range(1000):\n",
    "  client_start = time.time()\n",
    "  output = predictor.predict(data)\n",
    "  client_end = time.time()\n",
    "  client_times.append((client_end - client_start)*1000)\n",
    "cw_end = datetime.datetime.utcnow()\n",
    "\n",
    "print('Client end-to-end latency percentiles:')\n",
    "client_avg = np.mean(client_times)\n",
    "client_p50 = np.percentile(client_times, 50)\n",
    "client_p90 = np.percentile(client_times, 90)\n",
    "client_p95 = np.percentile(client_times, 95)\n",
    "client_p100 = np.percentile(client_times, 100)\n",
    "print('Avg | P50 | P90 | P95 | P100')\n",
    "print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(client_avg, client_p50, client_p90, client_p95, client_p100))\n",
    "\n",
    "print('Getting Cloudwatch:')\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "statistics=['SampleCount', 'Average', 'Minimum', 'Maximum']\n",
    "extended=['p50', 'p90', 'p95', 'p100']\n",
    "\n",
    "# Give 5 minute buffer to end\n",
    "cw_end += datetime.timedelta(minutes=5)\n",
    "\n",
    "# Period must be 1, 5, 10, 30, or multiple of 60\n",
    "# Calculate closest multiple of 60 to the total elapsed time\n",
    "factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "period = factor * 60\n",
    "print('Time elapsed: {} seconds'.format((cw_end - cw_start).total_seconds()))\n",
    "print('Using period of {} seconds\\n'.format(period))\n",
    "\n",
    "cloudwatch_ready = False\n",
    "# Keep polling CloudWatch metrics until datapoints are available\n",
    "while not cloudwatch_ready:\n",
    "  time.sleep(30)\n",
    "  print('Waiting 30 seconds ...')\n",
    "  # Must use default units of microseconds\n",
    "  model_latency_metrics = cloudwatch.get_metric_statistics(MetricName='ModelLatency',\n",
    "                                             Dimensions=[{'Name': 'EndpointName',\n",
    "                                                          'Value': endpoint_name},\n",
    "                                                         {'Name': 'VariantName',\n",
    "                                                          'Value': \"AllTraffic\"}],\n",
    "                                             Namespace=\"AWS/SageMaker\",\n",
    "                                             StartTime=cw_start,\n",
    "                                             EndTime=cw_end,\n",
    "                                             Period=period,\n",
    "                                             Statistics=statistics,\n",
    "                                             ExtendedStatistics=extended\n",
    "                                             )\n",
    "\n",
    "  # Should be 1000\n",
    "  if len(model_latency_metrics['Datapoints']) > 0:\n",
    "    print('{} latency datapoints ready'.format(model_latency_metrics['Datapoints'][0]['SampleCount']))\n",
    "    print('Side-car latency percentiles:')\n",
    "    side_avg = model_latency_metrics['Datapoints'][0]['Average'] / 1000\n",
    "    side_p50 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p50'] / 1000\n",
    "    side_p90 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p90'] / 1000\n",
    "    side_p95 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p95'] / 1000\n",
    "    side_p100 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p100'] / 1000\n",
    "    print('Avg | P50 | P90 | P95 | P100')\n",
    "    print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(side_avg, side_p50, side_p90, side_p95, side_p100))\n",
    "\n",
    "    cloudwatch_ready = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
